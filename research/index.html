<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  

























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Research | ViTAL Lab</title>

<link rel="icon" href="/images/icon.png">

<meta name="title" content="Research">
<meta name="description" content="Computational Beha**Vi**or and Heal**T**h **A**na**L**ytics (**ViTAL**) Lab at Emory Biomedical Informatics strives to develop artificial intelligence (AI) systems that are inclusive, accessible, fair, and reliable that will effectively improve the healthcare system. Our mission is to develop Ubiquitous Computing, Computer Vision, and Machine Learning systems using distributed ambient, mobile, and wearable devices to monitor patients' conditions in hospitals or everyday life. We are also invested in deploying and testing the developed AI systems in real-world clinical and daily living environments actively collaborating with stakeholders in healthcare.">

<meta property="og:title" content="Research">
<meta property="og:site_title" content="ViTAL Lab">
<meta property="og:description" content="Computational Beha**Vi**or and Heal**T**h **A**na**L**ytics (**ViTAL**) Lab at Emory Biomedical Informatics strives to develop artificial intelligence (AI) systems that are inclusive, accessible, fair, and reliable that will effectively improve the healthcare system. Our mission is to develop Ubiquitous Computing, Computer Vision, and Machine Learning systems using distributed ambient, mobile, and wearable devices to monitor patients' conditions in hospitals or everyday life. We are also invested in deploying and testing the developed AI systems in real-world clinical and daily living environments actively collaborating with stakeholders in healthcare.">
<meta property="og:url" content="">
<meta property="og:image" content="/images/share.png">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Research">
<meta property="twitter:description" content="Computational Beha**Vi**or and Heal**T**h **A**na**L**ytics (**ViTAL**) Lab at Emory Biomedical Informatics strives to develop artificial intelligence (AI) systems that are inclusive, accessible, fair, and reliable that will effectively improve the healthcare system. Our mission is to develop Ubiquitous Computing, Computer Vision, and Machine Learning systems using distributed ambient, mobile, and wearable devices to monitor patients' conditions in hospitals or everyday life. We are also invested in deploying and testing the developed AI systems in real-world clinical and daily living environments actively collaborating with stakeholders in healthcare.">
<meta property="twitter:url" content="">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/images/share.png">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Research",
    "description": "Computational Beha**Vi**or and Heal**T**h **A**na**L**ytics (**ViTAL**) Lab at Emory Biomedical Informatics strives to develop artificial intelligence (AI) systems that are inclusive, accessible, fair, and reliable that will effectively improve the healthcare system. Our mission is to develop Ubiquitous Computing, Computer Vision, and Machine Learning systems using distributed ambient, mobile, and wearable devices to monitor patients' conditions in hospitals or everyday life. We are also invested in deploying and testing the developed AI systems in real-world clinical and daily living environments actively collaborating with stakeholders in healthcare.",
    "headline": "Research",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/images/icon.png" }
    },
    "url": ""
  }
</script>

<link rel="alternate" type="application/rss+xml" href="/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.4.2/css/all.css" rel="preload" as="style" onload="this.onload = null; this.rel = 'stylesheet';">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.4.2/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/_styles/all.css" rel="stylesheet">
  

  
    <link href="/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/_styles/background.css" rel="stylesheet">
  

  
    <link href="/_styles/body.css" rel="stylesheet">
  

  
    <link href="/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/_styles/button.css" rel="stylesheet">
  

  
    <link href="/_styles/card.css" rel="stylesheet">
  

  
    <link href="/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/_styles/code.css" rel="stylesheet">
  

  
    <link href="/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/_styles/float.css" rel="stylesheet">
  

  
    <link href="/_styles/font.css" rel="stylesheet">
  

  
    <link href="/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/_styles/form.css" rel="stylesheet">
  

  
    <link href="/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/_styles/header.css" rel="stylesheet">
  

  
    <link href="/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/_styles/image.css" rel="stylesheet">
  

  
    <link href="/_styles/link.css" rel="stylesheet">
  

  
    <link href="/_styles/list.css" rel="stylesheet">
  

  
    <link href="/_styles/main.css" rel="stylesheet">
  

  
    <link href="/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/_styles/section.css" rel="stylesheet">
  

  
    <link href="/_styles/table.css" rel="stylesheet">
  

  
    <link href="/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/_scripts/anchors.js"></script>

  <script src="/_scripts/dark-mode.js"></script>

  <script src="/_scripts/fetch-tags.js"></script>

  <script src="/_scripts/search.js"></script>

  <script src="/_scripts/site-search.js"></script>

  <script src="/_scripts/tooltip.js"></script>


<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

  <body>
    




<header class="background" style="--image: url('')" data-dark="false">
  <a href="/" class="home">
    
    
      <span class="title" data-tooltip="Home">
        
          <span>ViTAL Lab</span>
        
        
          <span>Computational Behavior and Health Analytics</span>
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/research/" data-tooltip="Research Projects">
          Research
        </a>
      
    
      
        <a href="/publications/" data-tooltip="Published works">
          Publications
        </a>
      
    
      
        <a href="/teaching/" data-tooltip="Courses">
          Teaching
        </a>
      
    
      
        <a href="/career/" data-tooltip="Career">
          Join
        </a>
      
    
      
        <a href="/contact/" data-tooltip="Email, address, and location">
          Contact
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - wrap each table in div to allow for scrolling
  - filter out blank sections
-->








  
  
  

  <section class="background" data-size="page">
    <h1 class="center" id="research-projects">Research Projects</h1>

<div class="button-wrapper">
    <a class="button" href="/research/#generative-ai-and-cross-modality-techniques-for-human-activity-recognition" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Human Activity Recognition</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#smart-hospital-using-edge-computing-and-machine-learning-framework-with-multi-modal-ambient-mobile-and-wearable-sensors" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Smart Hospital with Ubiquitous AI</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#intelligent-telehealth-platforms-with-fair-artificial-intelligence-and-cloud-computing" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Telehealth with Fair AI</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#behavior-analytics-for-health-assessments" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Health Analytics with XAI</span>
      
    </a>
  </div>

<!-- [Machine Learning for Human Activity Recognition](#machine-learning-for-human-activity-recognition) 

[Edge Computing and Machine Learning Framework Using Multi-modal Ambient, Mobile, and Wearable Sensors](#edge-computing-and-machine-learning-framework-using-multi-modal-ambient-mobile-and-wearable-sensors)

[Behavior Analytics for Health Assessments](#behavior-analytics-for-health-assessments)
-->
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="generative-ai-and-cross-modality-techniques-for-human-activity-recognition">Generative AI and Cross-modality Techniques for Human Activity Recognition</h2>

<h3 id="opportunistic-use-of-video-data-for-wearable-based-human-activity-recognition">Opportunistic Use of Video Data for Wearable-based Human Activity Recognition</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/IMUTube%20Diagrams.jpg" style="
        width: 50%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Building human activity recognition model involves training a machine learning model to classify the given sensor signal input for target activities. The performance of the supervised learning approach largely depends on the scale of the training dataset that covers diverse activity patterns in the wild. However, collecting wearable sensor data from users is expensive, and annotation is time-consuming. Thereby, wearable datasets are typically limited in scale. To overcome the challenges in collecting wearable datasets, We proposed a method that uses a virtually unlimited amount of video data in online repositories to generate a training sensor dataset. The proposed technique uses state-of-the-art computer vision algorithms to track full human motions from human activity video and extract virtual inertial measurement signals from the on-body locations. The collection of virtual sensors from target activity videos is used to train the human activity recognition model, which is deployed in the real world. The proposed technique opens up the opportunity to apply complex analysis models for human activity recognition with the availability of large-scale training data</p>

<ul>
  <li>Kwon, H., Tong, C., Haresamudram, H., Gao, Y., Abowd, G. D., Lane, N. D., &amp; Ploetz, T. (2020). IMUTube: Automatic extraction of virtual on-body accelerometry from video for human activity recognition. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(3), 1-29.</li>
  <li>Kwon, H., Wang, B., Abowd, G. D., &amp; Plötz, T. (2021). Approaching the Real-World: Supporting Activity Recognition Training with Virtual IMU Data. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 5(3), 1-32.</li>
  <li>Kwon, H., Abowd, G. D., &amp; Plötz, T. (2021). Complex Deep Neural Networks from Large Scale Virtual IMU Data for Effective Human Activity Recognition Using Wearables. Sensors, 21(24), 8337.</li>
</ul>

<h3 id="large-language-model-and-generative-motion-model-for-wearable-based-human-activity-recognition">Large Language Model and Generative Motion Model for Wearable-based Human Activity Recognition</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/gpt-iswc.png" style="
        width: 50%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>The development of robust, generalized models in human activity recognition (HAR) has been hindered by the scarcity of large-scale, labeled data sets. Recent work has shown that virtual IMU data extracted from videos using computer vision techniques can lead to substantial performance improvements when training HAR models combined with small portions of real IMU data. Inspired by recent advances in motion synthesis from textual descriptions and connecting Large Language Models (LLMs) to various AI models, we introduce an automated pipeline that first uses ChatGPT to generate diverse textual descriptions of activities. These textual descriptions are then used to generate 3D human motion sequences via a motion synthesis model, T2M-GPT, and later converted to streams of virtual IMU data. We benchmarked our approach on three HAR datasets (RealWorld, PAMAP2, and USC-HAD) and demonstrate that the use of virtual IMU training data generated using our new approach leads to significantly improved HAR model performance compared to only using real IMU data. Our approach contributes to the growing field of cross-modality transfer methods and illustrate how HAR models can be improved through the generation of virtual training data that do not require any manual effort.</p>

<ul>
  <li>Zikang Leng, Hyeokhyen Kwon, and Thomas Ploetz. 2023. Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition. In Proceedings of the 2023 International Symposium on Wearable Computers (ISWC ‘23). Association for Computing Machinery, New York, NY, USA, 39–43. https://doi.org/10.1145/3594738.3611361</li>
  <li>Zikang Leng, Amitrajit Bhattacharjee, Hrudhai Rajasekhar, Lizhe Zhang, Elizabeth Bruda, Hyeokhyen Kwon, and Thomas Plötz. 2024. IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based Human Activity Recognition. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 8, 3, Article 112 (August 2024), 32 pages. https://doi.org/10.1145/3678545</li>
</ul>

<h3 id="robust-human-activity-recognition-model-with-uncertainty-modeling">Robust Human Activity Recognition Model with Uncertainty Modeling</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/label%20jitter.png" style="
        width: 80%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Building human activity recognition model involves training a machine learning model to classify the given sensor signal feature for target activities. For the conventional supervised learning approach, discriminative sensor feature representation and accurate activity annotations are necessary to learn the robust human activity recognition model. However, acquiring accurate activity annotation is intractable due to the continuous nature of human activities. Thereby, the activity annotations have uncertainty regarding the correct temporal alignment of activity boundaries and the correctness of the actual label. We explore novel machine learning techniques to handle such uncertainties.</p>

<ul>
  <li>Kwon, H., Abowd, G. D., &amp; Plötz, T. (2018, October). Adding structural characteristics to distribution-based accelerometer representations for activity recognition using wearables. In Proceedings of the 2018 ACM international symposium on wearable computers (pp. 72-75).</li>
  <li>Kwon, H., Abowd, G. D., &amp; Plötz, T. (2019, September). Handling annotation uncertainty in human activity recognition. In Proceedings of the 23rd International Symposium on Wearable Computers (pp. 109-117).</li>
</ul>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="smart-hospital-using-edge-computing-and-machine-learning-framework-with-multi-modal-ambient-mobile-and-wearable-sensors">Smart Hospital using Edge Computing and Machine Learning Framework with Multi-modal Ambient, Mobile, and Wearable Sensors</h2>

<h3 id="low-cost-passive-and-continuous-patient-monitoring-system-using-artificial-intelligence-distributed-multi-view-camera-system-and-wearables">Low-cost, Passive, and Continuous Patient Monitoring System Using Artificial Intelligence, Distributed Multi-view Camera System, and Wearables</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/cep_ep6_cam_wear.png" style="
        width: 90%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Neurological disorders, like Alzhemer’s Disease or related dementia (ADRD), can influence an individual’s mobility and social interactions, offering insights into disease progression. Yet, accurately measuring intricate movements and social interactions in real-world settings is challenging, making clinicians rely on retrospective surveys. Addressing this challenge, I’ve introduced a cost-effective, passive, continuous patient-monitoring system capable of quantifying patients’ social behaviors and movements within therapeutic facilities spanning 18,000 sqft. This system integrates multiple edge computing devices, combined with AI, cameras, and wearable sensors, to localize, track, and identify group activities. Specifically, it’s applied to quantify Mild Cognitive Impairment (MCI), a preceding condition of ADRD. With minimal modification to the existing infrastructure, this system can transform standard therapeutic areas into smart environments that can evaluate conditions linked to neurological disorders at a low cost.</p>

<ul>
  <li>H. Kwon et al., “A Feasibility Study on Indoor Localization and Multi-person Tracking Using Sparsely Distributed Camera Network with Edge Computing,” in IEEE Journal of Indoor and Seamless Positioning and Navigation, doi: 10.1109/JISPIN.2023.3337189.</li>
  <li>C. Hegde et al., “Indoor Group Identification and Localization Using Privacy-Preserving Edge Computing Distributed Camera Network,” in IEEE Journal of Indoor and Seamless Positioning and Navigation, doi: 10.1109/JISPIN.2024.3354248.</li>
  <li>Kiarashi et al., “Graph Trilateration for Indoor Localization in Sparsely Distributed Edge Computing Devices in Complex Environments Using Bluetooth Technology,” Sensors 2023, 23, 9517.</li>
</ul>

<h3 id="transforming-education-space-using-multimodal-artificial-intelligence-using-camera-audio-and-wearables">Transforming Education Space Using Multimodal Artificial Intelligence using Camera, Audio, and Wearables</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/tcfd-vision-group-binary.png" style="
        width: 60%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Rapid identification and accurate documentation of interfering and high-risk behaviors in ASD, such as aggression, self-injury, disruption, and restricted repetitive behaviors, are important in daily classroom environments for tracking intervention effectiveness and allocating appropriate resources to manage care needs. However, having a staff dedicated solely to observing is costly and uncommon in most educational settings. Recently, multiple research studies have explored developing automated, continuous, and objective tools using machine learning models to quantify behaviors in ASD. However, the majority of the work was conducted under a controlled environment and has not been validated for real-world conditions. In this work, we demonstrate that the latest advances in video-based group activity recognition techniques can quantify behaviors in ASD in real-world activities in classroom environments while preserving privacy. Our explainable model could detect the episode of problem behaviors with a 77% F1-score and capture distinctive behavior features in different types of behaviors in ASD. To the best of our knowledge, this is the first work that shows the promise of objectively quantifying behaviors in ASD in a real-world environment, which is an important step toward the development of a practical tool that can ease the burden of data collection for classroom staff.</p>

<ul>
  <li>Barun Das, Conor Anderson, Tania Villavicencio, Johanna Lantz, Jenny Foster, Theresa Hamlin, Ali Bahrami Rad, Gari D. Clifford, Hyeokhyen Kwon (2024). Explainable Artificial Intelligence for Quantifying Interfering and High-Risk Behaviors in Autism Spectrum Disorder in a Real-World Classroom Environment Using Privacy-Preserving Video Analysis. arXiv:2407.21691</li>
</ul>

<h3 id="transforming-movement-disorder-clinics-using-explainable-artificial-intelligence-for-motion-capture">Transforming Movement Disorder Clinics Using Explainable Artificial Intelligence for Motion Capture</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/xai4fog.png" style="
        width: 60%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Traditional methods for diagnosing and assessing Parkinson’s disease (PD) often involve thorough physical examinations by specialists in movement disorders. These methods, including the use of scales like MDS-UPDRS or patient questionnaires (N-FOG-Q), can be subjective and vary between different raters. Particularly challenging is assessing Freezing of Gait (FOG), a complex aspect of parkinsonism that significantly affects quality of life. PD is also known to manifest in different motor subtypes, mainly tremor dominant (TD) and postural instability/gait difficulty (PIGD), each presenting unique challenges in balance and gait. Previous studies have employed machine learning to analyze kinematic data from body movements during gait assessments, but these often lacked the detail necessary to fully capture the complexity of whole-body movements in PD. To address this, a new method using explainable artificial intelligence models has been developed. These models can objectively and accurately measure PD, mirroring the assessment capabilities of human specialists. They show promise in accurately identifying FOG severity levels and PD subtypes, thereby aiding in the early identification of motor subtypes and predicting disease progression. Moreover, these models can potentially discover new kinematic biomarkers for PD, offering valuable insights for clinical trials and hypothesis generation.</p>

<ul>
  <li>Kwon, H., Clifford, G. D., Genias, I., Bernhard, D., Esper, C. D., Factor, S. A., &amp; McKay, J. L. (2023). An explainable spatial-temporal graphical convolutional network to score freezing of gait in parkinsonian patients. Sensors, 23(4), 1766.</li>
  <li>Gong, N. J., Clifford, G. D., Esper, C. D., Factor, S. A., McKay, J. L., &amp; Kwon, H. (2023). Classifying Tremor Dominant and Postural Instability and Gait Difficulty Subtypes of Parkinson’s Disease from Full-Body Kinematics. Sensors, 23(19), 8330.</li>
  <li>Saad, M., Hefner, S., Donovan, S., Bernhard, D., Tripathi, R., Factor, S.A., Powell, J.M., Kwon, H., Sameni, R., Esper, C.D. and McKay, J.L., 2024. Development of a Tremor Detection Algorithm for Use in an Academic Movement Disorders Center. Sensors, 24(15), p.4960.</li>
</ul>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="intelligent-telehealth-platforms-with-fair-artificial-intelligence-and-cloud-computing">Intelligent Telehealth Platforms with Fair Artificial Intelligence and Cloud Computing</h2>

<h3 id="multi-modal-machine-learning-system-for-assessing-remote-interviews">Multi-modal Machine Learning System for Assessing Remote Interviews</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/multi-modal-MH.png" style="
        width: 60%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>This research addresses the challenges in current psychiatric evaluations, which often suffer from subjectivity, bias, and the need for highly skilled professionals. The study explores the use of objective digital biomarkers, derived from behavioral and physiological signals in remote interviews, for psychiatric disorder assessment. These signals include facial and vocal expressions, linguistic content, and cardiovascular modulation. The study used a multimodal approach, evaluating each modality individually and in combination, to assess mental health status. This approach aims to make mental health assessments more objective, remote, and cost-effective. The research also critically examines the fairness of these automated mental health assessment tools across different demographics (race, gender, education level, and age). Despite their growing importance, especially in telehealth, these tools can carry biases that may lead to unfair treatment of certain groups. The study systematically evaluated and discussed the need for reporting and mitigating these biases to build trust and ensure appropriate application in clinical settings. Using a developed multimodal mental health assessment system, the study analyzed how various features (facial expressions, voice acoustics, linguistic expressions, and cardiovascular patterns) affected fairness across demographics. The findings indicated that no single modality was consistently fair for all groups. Although methods to mitigate unfairness improved fairness levels, they also highlighted a trade-off between performance and fairness. This calls for a broader moral discussion and further investigation into the ethics and application of automated mental health assessment tools.</p>

<ul>
  <li>Jiang, Z., Seyedi, S., Griner, E., Abbasi, A., Rad, A.B., Kwon, H., Cotes, R.O. and Clifford, G.D., 2024. Multimodal Mental Health Digital Biomarker Analysis from Remote Interviews using Facial, Vocal, Linguistic, and Cardiovascular Patterns. IEEE Journal of Biomedical and Health Informatics.</li>
  <li>Jiang, Z., Seyedi, S., Griner, E., Abbasi, A., Rad, A.B., Kwon, H., Cotes, R.O. and Clifford, G.D. (2024) Evaluating and mitigating unfairness in multimodal remote mental health assessments. PLOS Digital Health 3(7): e0000413. https://doi.org/10.1371/journal.pdig.0000413</li>
</ul>

<h3 id="mobile-artificial-intelligence-systems-for-assessing-gait-impairment">Mobile Artificial Intelligence Systems for Assessing Gait Impairment</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/mobile_phone_gait.png" style="
        width: 80%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Accurate diagnosis of gait impairments is often hindered by subjective or costly
assessment methods, with current solutions requiring either expensive multi-camera
equipment or relying on subjective clinical observation. There is a critical need for
accessible, objective tools that can aid in gait assessment while preserving patient
privacy. In this work, we present a mobile phone-based, privacy-preserving artificial
intelligence (AI) system for classifying gait impairments and introduce a novel dataset
of 743 videos capturing seven distinct gait patterns. The dataset consists of frontal and
sagittal views of trained subjects simulating normal gait and six types of pathological
gait (circumduction, Trendelenburg, antalgic, crouch, Parkinsonian, and vaulting),
recorded using standard mobile phone cameras. Our system achieved 86.5% accuracy
using combined frontal and sagittal views, with sagittal views generally outperforming
frontal views except for specific gait patterns like Circumduction. Model feature
importance analysis revealed that frequency-domain features and entropy measures were
critical for classifcation performance, specifically lower limb keypoints proved most
important for classification, aligning with clinical understanding of gait assessment.
These findings demonstrate that mobile phone-based systems can effectively classify
diverse gait patterns while preserving privacy through on-device processing. The high
accuracy achieved using simulated gait data suggests their potential for rapid
prototyping of gait analysis systems, though clinical validation with patient data
remains necessary. This work represents a significant step toward accessible, objective
gait assessment tools for clinical, community, and tele-rehabilitation settings.</p>

<ul>
  <li>Reddy, Lauhitya, Ketan Anand, Shoibolina Kaushik, Corey Rodrigo, J. Lucas McKay, Trisha M. Kesar, and Hyeokhyen Kwon. “Classifying Simulated Gait Impairments using Privacy-preserving Explainable Artificial Intelligence and Mobile Phone Videos.” arXiv preprint arXiv:2412.01056 (2024).</li>
</ul>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="behavior-analytics-for-health-assessments">Behavior Analytics for Health Assessments</h2>

<h3 id="parkinsons-disease">Parkinson’s Disease</h3>

<p>Refer <a href="/research/#transforming-movement-disorder-clinics-using-explainable-artificial-intelligence-for-motion-capture">“Movement clinc with AI”</a></p>

<h3 id="stroke-survivors">Stroke survivors</h3>

<p>Refer <a href="/research/#mobile-artificial-intelligence-systems-for-assessing-gait-impairment">Mobile AI for Stroke</a></p>

<h3 id="mental-health-and-depression">Mental Health and Depression</h3>

<p>Refer <a href="/research/#multi-modal-machine-learning-system-for-assessing-remote-interviews">“Multi-modal AI System”</a></p>

<h3 id="autism-spectrum-disorder">Autism Spectrum Disorder</h3>

<p>Refer <a href="/research/#transforming-education-space-using-multimodal-artificial-intelligence-using-camera-audio-and-wearables">“Classroom with AI”</a></p>

<h3 id="cognitive-impairment-alzheimers-disease-and-dementia">Cognitive Impairment, Alzheimer’s Disease, and Dementia</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/publications/cep_ep6_vision_only_moca.png" style="
        width: 50%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Mild cognitive impairment (MCI) is characterized by a decline in cognitive functions beyond typical age and education-related expectations. Since, MCI has been linked to reduced social interactions and increased aimless movements, we aimed to automate the capture of these behaviors to enhance longitudinal monitoring.
Using a privacy-preserving distributed camera network, we collected movement and social interaction data from groups of individuals with MCI undergoing therapy within a 1700$m^2$ space. 
We developed movement and social interaction features, which were then used to train a series of machine learning algorithms to distinguish between higher and lower cognitive functioning MCI groups.
A Wilcoxon rank-sum test revealed statistically significant differences between high and low-functioning cohorts in features such as linear path length, walking speed, change in direction while walking, entropy of velocity and direction change, and number of group formations in the indoor space.
Despite lacking individual identifiers to associate with specific levels of MCI, a machine learning approach using the most significant features provided a 71\% accuracy.<br>
We provide evidence to show that a privacy-preserving low-cost camera network using edge computing framework has the potential to distinguish between different levels of cognitive impairment from the movements and social interactions captured during group activities.</p>

<ul>
  <li>Hegde, Chaitra, Yashar Kiarashi, Allan I. Levey, Amy D. Rodriguez, Hyeokhyen Kwon, and Gari D. Clifford. “Feasibility of assessing cognitive impairment via distributed camera network and privacy-preserving edge computing.” arXiv preprint arXiv:2408.10442 (2024).</li>
</ul>
  </section>


    </main>
    


<footer class="background" style="--image: url('')" data-dark="false" data-size="wide">
  <div>
    <!--
      Extra details like contact info or address
    -->

    <img src="../images/EU_shield_hz_280.jpg" height="50">
           
    <img src="../images/GeorgiaTech_RGB.png" height="60">
    <!-- &nbsp; &nbsp; &nbsp; &nbsp;
    <img
    src="../images/GaTechEmory_WHBME.jpg"
    height="60"
    >
    &nbsp; &nbsp; &nbsp; &nbsp;
    <img
    src="../images/BMI_hori.png"
    height="60"
    >
    &nbsp; &nbsp; &nbsp; &nbsp;
    <img
    src="../images/bme-logo-gold_0.png"
    height="60"
    > -->

  </div>

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="mailto:hyeokhyen.kwon@emory.edu" data-tooltip="Email" data-style="bare" aria-label="Email">
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://orcid.org/0000-0002-5693-3278" data-tooltip="ORCID" data-style="bare" aria-label="ORCID">
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=OwLXC4YAAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/KwonVitalLab" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://twitter.com/HyeokhyenKwon" data-tooltip="X/Twitter" data-style="bare" aria-label="X/Twitter">
      <i class="icon fa-brands fa-x-twitter"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2025
    ViTAL Lab
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
