<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  

























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Research | ViTAL Lab</title>

<link rel="icon" href="/images/icon.png">

<meta name="title" content="Research">
<meta name="description" content="Computational Beha**Vi**or and Heal**T**h **A**na**L**ytics (**ViTAL**) Lab at Emory Biomedical Informatics strives to develop artificial intelligence (AI) systems that are inclusive, accessible, fair, and reliable that will effectively improve the healthcare system. Our mission is to develop Ubiquitous Computing, Computer Vision, and Machine Learning systems using distributed ambient, mobile, and wearable devices to monitor patients' conditions in hospitals or everyday life. We are also invested in deploying and testing the developed AI systems in real-world clinical and daily living environments actively collaborating with stakeholders in healthcare.">

<meta property="og:title" content="Research">
<meta property="og:site_title" content="ViTAL Lab">
<meta property="og:description" content="Computational Beha**Vi**or and Heal**T**h **A**na**L**ytics (**ViTAL**) Lab at Emory Biomedical Informatics strives to develop artificial intelligence (AI) systems that are inclusive, accessible, fair, and reliable that will effectively improve the healthcare system. Our mission is to develop Ubiquitous Computing, Computer Vision, and Machine Learning systems using distributed ambient, mobile, and wearable devices to monitor patients' conditions in hospitals or everyday life. We are also invested in deploying and testing the developed AI systems in real-world clinical and daily living environments actively collaborating with stakeholders in healthcare.">
<meta property="og:url" content="">
<meta property="og:image" content="/images/share.png">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Research">
<meta property="twitter:description" content="Computational Beha**Vi**or and Heal**T**h **A**na**L**ytics (**ViTAL**) Lab at Emory Biomedical Informatics strives to develop artificial intelligence (AI) systems that are inclusive, accessible, fair, and reliable that will effectively improve the healthcare system. Our mission is to develop Ubiquitous Computing, Computer Vision, and Machine Learning systems using distributed ambient, mobile, and wearable devices to monitor patients' conditions in hospitals or everyday life. We are also invested in deploying and testing the developed AI systems in real-world clinical and daily living environments actively collaborating with stakeholders in healthcare.">
<meta property="twitter:url" content="">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/images/share.png">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Research",
    "description": "Computational Beha**Vi**or and Heal**T**h **A**na**L**ytics (**ViTAL**) Lab at Emory Biomedical Informatics strives to develop artificial intelligence (AI) systems that are inclusive, accessible, fair, and reliable that will effectively improve the healthcare system. Our mission is to develop Ubiquitous Computing, Computer Vision, and Machine Learning systems using distributed ambient, mobile, and wearable devices to monitor patients' conditions in hospitals or everyday life. We are also invested in deploying and testing the developed AI systems in real-world clinical and daily living environments actively collaborating with stakeholders in healthcare.",
    "headline": "Research",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/images/icon.png" }
    },
    "url": ""
  }
</script>

<link rel="alternate" type="application/rss+xml" href="/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.4.2/css/all.css" rel="preload" as="style" onload="this.onload = null; this.rel = 'stylesheet';">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.4.2/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/_styles/all.css" rel="stylesheet">
  

  
    <link href="/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/_styles/background.css" rel="stylesheet">
  

  
    <link href="/_styles/body.css" rel="stylesheet">
  

  
    <link href="/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/_styles/button.css" rel="stylesheet">
  

  
    <link href="/_styles/card.css" rel="stylesheet">
  

  
    <link href="/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/_styles/code.css" rel="stylesheet">
  

  
    <link href="/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/_styles/float.css" rel="stylesheet">
  

  
    <link href="/_styles/font.css" rel="stylesheet">
  

  
    <link href="/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/_styles/form.css" rel="stylesheet">
  

  
    <link href="/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/_styles/header.css" rel="stylesheet">
  

  
    <link href="/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/_styles/image.css" rel="stylesheet">
  

  
    <link href="/_styles/link.css" rel="stylesheet">
  

  
    <link href="/_styles/list.css" rel="stylesheet">
  

  
    <link href="/_styles/main.css" rel="stylesheet">
  

  
    <link href="/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/_styles/section.css" rel="stylesheet">
  

  
    <link href="/_styles/table.css" rel="stylesheet">
  

  
    <link href="/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/_scripts/anchors.js"></script>

  <script src="/_scripts/dark-mode.js"></script>

  <script src="/_scripts/fetch-tags.js"></script>

  <script src="/_scripts/search.js"></script>

  <script src="/_scripts/site-search.js"></script>

  <script src="/_scripts/tooltip.js"></script>


</head>

  <body>
    




<header class="background" style="--image: url('')" data-dark="false">
  <a href="/" class="home">
    
    
      <span class="title" data-tooltip="Home">
        
          <span>ViTAL Lab</span>
        
        
          <span>Computational Behavior and Health Analytics</span>
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/news/" data-tooltip="News">
          News
        </a>
      
    
      
        <a href="/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/research/" data-tooltip="Research Projects">
          Research
        </a>
      
    
      
        <a href="/publications/" data-tooltip="Published works">
          Publications
        </a>
      
    
      
        <a href="/teaching/" data-tooltip="Courses">
          Teaching
        </a>
      
    
      
        <a href="/career/" data-tooltip="Career">
          Join
        </a>
      
    
      
        <a href="/contact/" data-tooltip="Email, address, and location">
          Contact
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - wrap each table in div to allow for scrolling
  - filter out blank sections
-->








  
  
  

  <section class="background" data-size="page">
    <h1 class="center" id="research-projects">Research Projects</h1>

<div class="button-wrapper">
    <a class="button" href="/research/#generative-ai-and-cross-modality-techniques-for-human-activity-recognition" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Human Activity Recognition</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#behavior-sensing-in-clinics-using-ambient-ai-with-video-and-wearable-sensors" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Ambient AI in Clinic</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#behavior-sensing-with-wearables-at-home" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Wearable AI at Home</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#mobile-computer-vision-and-closed-loop-intervention-systems-for-motion-assessments-and-rehabilitation" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Mobile Computer Vision</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#faical-expression-speech-language-cardiovascular-signal-analysis-in-remote-mental-health-interviews" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Multimodal AI for Mental Health Interview</span>
      
    </a>
  </div>

<!-- [Machine Learning for Human Activity Recognition](#machine-learning-for-human-activity-recognition) 

[Edge Computing and Machine Learning Framework Using Multi-modal Ambient, Mobile, and Wearable Sensors](#edge-computing-and-machine-learning-framework-using-multi-modal-ambient-mobile-and-wearable-sensors)

[Behavior Analytics for Health Assessments](#behavior-analytics-for-health-assessments)
-->
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="generative-ai-and-cross-modality-techniques-for-human-activity-recognition">Generative AI and Cross-modality Techniques for Human Activity Recognition</h2>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/har.png" style="
        width: 70%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>In addressing the challenges of building a human activity recognition model with limited wearable sensor data, we proposed a novel method utilizing vast video and textual data resources to generate a large-scale training dataset. This cross-modality transfer learning technique harnesses state-of-the-art computer vision algorithms to interpret human motions from videos or Large Language Models to create text descriptions of human activities, eventually generating virtual inertial measurement signals that can be used for model training. The resulting model, trained on this rich dataset, can tackle real-world applications. Additionally, we introduced an innovative training scheme to account for the inherent uncertainties in activity annotations. This is complemented by a novel sensor feature representation that effectively captures the structural and distributional nuances of sensor signals, capturing human activity, enhancing the model’s robustness.</p>

<ul>
  <li>Kwon, H., Tong, C., Haresamudram, H., Gao, Y., Abowd, G. D., Lane, N. D., &amp; Ploetz, T. (2020). IMUTube: Automatic extraction of virtual on-body accelerometry from video for human activity recognition. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(3), 1-29.</li>
  <li>Zikang Leng, Amitrajit Bhattacharjee, Hrudhai Rajasekhar, Lizhe Zhang, Elizabeth Bruda, Hyeokhyen Kwon, and Thomas Plötz. 2024. IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based Human Activity Recognition. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 8, 3, Article 112 (August 2024), 32 pages. https://doi.org/10.1145/3678545</li>
  <li>Kwon, H., Abowd, G. D., &amp; Plötz, T. (2018, October). Adding structural characteristics to distribution-based accelerometer representations for activity recognition using wearables. In Proceedings of the 2018 ACM international symposium on wearable computers (pp. 72-75).</li>
  <li>Kwon, H., Abowd, G. D., &amp; Plötz, T. (2019, September). Handling annotation uncertainty in human activity recognition. In Proceedings of the 23rd International Symposium on Wearable Computers (pp. 109-117).</li>
</ul>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="behavior-sensing-in-clinics-using-ambient-ai-with-video-and-wearable-sensors">Behavior Sensing in Clinics using Ambient AI with Video and Wearable Sensors</h2>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/ambient.png" style="
        width: 70%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Our work explores how smart, privacy-preserving camera and sensor systems can monitor cognitive and social behaviors in real-world clinical settings. Focusing on mild cognitive impairment (MCI), we used edge-computing camera networks and Bluetooth beacons in a large therapeutic facility to detect differences in movement and social interactions between high- and low-functioning individuals. A novel method was also developed to identify when and where group activities occur in indoor spaces using lightweight pose detection, with high accuracy. The system was scalable, low-cost, and effective in localizing and tracking people while preserving privacy. Further, the 3D kinematics motion analysis technologies were adapted to objectively measure freezing of gait in Parkinson’s patients using explainable deep learning in a motion analysis laboratory in academic clinics. A video-based group activity analysis system could identify problematic behaviors in students with autism in real-world special education classroom. Together, these studies demonstrate the promise of edge AI systems for passive, real-time monitoring of cognitive and behavioral health across diverse populations and environments.</p>

<ul>
  <li>Hegde, C., Kiarashi, Y., Levey, A. I., Rodriguez, A. D., Kwon, H., &amp; Clifford, G. D. (2025). Feasibility of assessing cognitive impairment via distributed camera network and privacy‐preserving edge computing. Alzheimer’s &amp; Dementia: Diagnosis, Assessment &amp; Disease Monitoring, 17(1), e70085.</li>
  <li>Hegde, C., Kiarashi, Y., Rodriguez, A. D., Levey, A. I., Doiron, M., Kwon, H., &amp; Clifford, G. D. (2024). Indoor Group Identification and Localization Using Privacy-Preserving Edge Computing Distributed Camera Network. IEEE journal of indoor and seamless positioning and navigation, 2, 51-60.</li>
  <li>Kwon, H., Hegde, C., Kiarashi, Y., Madala, V. S. K., Singh, R., Nakum, A., … &amp; Clifford, G. D. (2023). A feasibility study on indoor localization and multiperson tracking using sparsely distributed camera network with edge computing. IEEE Journal of Indoor and Seamless Positioning and Navigation, 1, 187-198.</li>
  <li>Barun Das, Conor Anderson, Tania Villavicencio, Johanna Lantz, Jenny Foster, Theresa Hamlin, Ali Bahrami Rad, Gari D. Clifford, Hyeokhyen Kwon (2024). Explainable Artificial Intelligence for Quantifying Interfering and High-Risk Behaviors in Autism Spectrum Disorder in a Real-World Classroom Environment Using Privacy-Preserving Video Analysis. arXiv:2407.21691</li>
  <li>Kwon, H., Clifford, G. D., Genias, I., Bernhard, D., Esper, C. D., Factor, S. A., &amp; McKay, J. L. (2023). An explainable spatial-temporal graphical convolutional network to score freezing of gait in parkinsonian patients. Sensors, 23(4), 1766.</li>
</ul>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="behavior-sensing-with-wearables-at-home">Behavior Sensing with Wearables at Home</h2>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/wearables.png" style="
        width: 60%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>We are exploring how wearable sensors and thoughtful home design can help detect and support cognitive and behavioral challenges in people with conditions like Parkinson’s disease (PD), autism spectrum disorder (ASD), and mild cognitive impairment (MCI). In PD, we found that existing activity recognition models used to detect “freezing of gait” (FOG) can be biased by age, sex, and disease stage, but fairness and accuracy improved using transfer learning from diverse datasets. In children with ASD, wearable devices tracking movement and physiological signals like skin temperature were able to detect behaviors like self-injury in everyday settings, offering a real-world way to support care. For older adults with MCI, wearables helped identify functional differences in kitchen tasks—like weaker upper body movements and delayed eye movements—highlighting how behavior reflects cognitive decline. we also found that open kitchen shelving slightly improved motivation and task efficiency, though personal aesthetic preferences still mattered. Overall, these studies show how wearable technology and personalized environments can offer practical, real-world support for people facing cognitive and behavioral health challenges.</p>

<ul>
  <li>Odonga, T., Esper, C. D., Factor, S. A., McKay, J. L., &amp; Kwon, H. (2025). On the Bias, Fairness, and Bias Mitigation for a Wearable-based Freezing of Gait Detection in Parkinson’s Disease. arXiv preprint arXiv:2502.09626.</li>
  <li>Rad, A. B., Villavicencio, T., Kiarashi, Y., Anderson, C., Foster, J., Kwon, H., … &amp; Clifford, G. D. (2025). From motion to emotion: exploring challenging behaviors in autism spectrum disorder through analysis of wearable physiology and movement. Physiological Measurement, 13(1), 015004.</li>
  <li>Ibrahim Bilau, Bonwoo Koo, Esther Fu, Wendy Chau, Hyeokhyen Kwon, Eunhwa Yang, Visual Accessibility through Open Shelving: Impacts on Cognitive Load, Motivation, Physical Activity, and User Perception in Older Adults with Mild Cognitive Impairment, medRxiv 2025.05.21.25328033; doi: https://doi.org/10.1101/2025.05.21.25328033</li>
  <li>Bonwoo Koo, Ibrahim Bilau, Amy D. Rodriguez, Eunhwa Yang, Hyeokhyen Kwon, Quantifying Mild Cognitive Impairments in Older Adults Using Multi-modal Wearable Sensor Data in a Kitchen Environment, medRxiv 2025.05.24.25328107; doi: https://doi.org/10.1101/2025.05.24.25328107</li>
</ul>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="mobile-computer-vision-and-closed-loop-intervention-systems-for-motion-assessments-and-rehabilitation">Mobile Computer Vision and Closed-loop Intervention Systems for Motion Assessments and Rehabilitation</h2>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/mobileComputerVision.png" style="
        width: 60%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>We developed mobile AI systems to improve motor assessment and rehabilitation. A mobile phone-based, privacy-preserving system classifies various gait impairment patterns, demonstrating the potential for accessible gait analysis in clinical and tele-rehabilitation settings. We also developed a closed-loop neurostimulation system that integrates real-time video-based movement classification on webcam with wireless transcutaneous vagus nerve stimulation (tVNS), automatically triggering stimulation during rehabilitation exercises, showcasing a promising step toward patient-driven, home-based motor rehabilitation.</p>

<ul>
  <li>Reddy, Lauhitya, Ketan Anand, Shoibolina Kaushik, Corey Rodrigo, J. Lucas McKay, Trisha M. Kesar, and Hyeokhyen Kwon. “Classifying Simulated Gait Impairments using Privacy-preserving Explainable Artificial Intelligence and Mobile Phone Videos.” arXiv preprint arXiv:2412.01056 (2024).</li>
  <li>Minoru Shinohara, Arya Mohan, Nathaniel Green, Joshua N. Posen, Milka Trajkova, Woon-Hong Yeo, Hyeokhyen Kwon, “Closed-loop Neuromotor Training System Pairing Transcutaneous Vagus Nerve Stimulation with Video-based Real-time Movement Classification”, medRxiv 2025.05.23.25327218; doi: https://doi.org/10.1101/2025.05.23.25327218</li>
</ul>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="faical-expression-speech-language-cardiovascular-signal-analysis-in-remote-mental-health-interviews">Faical Expression, Speech, Language, cardiovascular signal Analysis in Remote Mental Health Interviews</h2>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/remote_interview.png" style="
        width: 80%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>We are developing novel machine learning tools that use audio, video, language, and heart rate data from remote interviews to help detect mental health conditions and cognitive decline in older adults. These tools can assess risks like dementia, depression, anxiety, social isolation, and personality traits like neuroticism, using features like facial expressions, speech patterns, and cardiovascular signals. Studies also showed that different data types—like speech for cognitive issues and heart rate for mental health—play unique roles. However, fairness remains a concern, as detection accuracy can vary across demographic groups like race, age, and education. While methods like post-training calibration can reduce these biases, they come with the cost of reduced performance. Overall, these tools show promise for affordable, remote mental health and cognitive screening, but continued attention to fairness and clinical integration is critical.</p>

<ul>
  <li>Jiang, Z., Seyedi, S., Griner, E., Abbasi, A., Rad, A.B., Kwon, H., Cotes, R.O. and Clifford, G.D., 2024. Multimodal Mental Health Digital Biomarker Analysis from Remote Interviews using Facial, Vocal, Linguistic, and Cardiovascular Patterns. IEEE Journal of Biomedical and Health Informatics.</li>
  <li>Jiang, Z., Seyedi, S., Griner, E., Abbasi, A., Rad, A.B., Kwon, H., Cotes, R.O. and Clifford, G.D. (2024) Evaluating and mitigating unfairness in multimodal remote mental health assessments. PLOS Digital Health 3(7): e0000413. https://doi.org/10.1371/journal.pdig.0000413</li>
  <li>Mu, X., Seyedi, S., Zheng, I., Jiang, Z., Chen, L., Omofojoye, B., … &amp; Kwon, H. (2024). Detecting Cognitive Impairment and Psychological Well-being among Older Adults Using Facial, Acoustic, Linguistic, and Cardiovascular Patterns Derived from Remote Conversations. arXiv preprint arXiv:2412.14194.</li>
  <li>Qin, R., Yang, K., Abbasi, A., Dobolyi, D., Seyedi, S., Griner, E., Kwon, H., Cotes, R., Jiang, Z., Clifford, G. and Cook, R.A., 2025. Language models for online depression detection: A review and benchmark analysis on remote interviews. ACM Transactions on Management Information Systems, 16(2), pp.1-35.</li>
</ul>
  </section>


    </main>
    


<footer class="background" style="--image: url('')" data-dark="false" data-size="wide">
  <div>
    <!--
      Extra details like contact info or address
    -->

    <img src="../images/EU_shield_hz_280.jpg" height="50">
           
    <img src="../images/GeorgiaTech_RGB.png" height="60">
    <!-- &nbsp; &nbsp; &nbsp; &nbsp;
    <img
    src="../images/GaTechEmory_WHBME.jpg"
    height="60"
    >
    &nbsp; &nbsp; &nbsp; &nbsp;
    <img
    src="../images/BMI_hori.png"
    height="60"
    >
    &nbsp; &nbsp; &nbsp; &nbsp;
    <img
    src="../images/bme-logo-gold_0.png"
    height="60"
    > -->

  </div>

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="mailto:hyeokhyen.kwon@emory.edu" data-tooltip="Email" data-style="bare" aria-label="Email">
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://orcid.org/0000-0002-5693-3278" data-tooltip="ORCID" data-style="bare" aria-label="ORCID">
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=OwLXC4YAAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/KwonVitalLab" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://twitter.com/HyeokhyenKwon" data-tooltip="X" data-style="bare" aria-label="X">
      <i class="icon fa-brands fa-x-twitter"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2025
    ViTAL Lab
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
