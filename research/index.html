<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  

























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Research | ViTAL Lab</title>

<link rel="icon" href="/images/icon.png">

<meta name="title" content="Research">
<meta name="description" content="Computational Beha**Vi**or and Heal**T**h **A**na**L**ytics (**ViTAL**) Lab at Emory Biomedical Informatics strives to develop artificial intelligence (AI) systems that are inclusive, accessible, fair, and reliable that will effectively improve the healthcare system. Our mission is to develop Ubiquitous Computing, Computer Vision, and Machine Learning systems using distributed ambient, mobile, and wearable devices to monitor patients' conditions in hospitals or everyday life. We are also invested in deploying and testing the developed AI systems in real-world clinical and daily living environments actively collaborating with stakeholders in healthcare.">

<meta property="og:title" content="Research">
<meta property="og:site_title" content="ViTAL Lab">
<meta property="og:description" content="Computational Beha**Vi**or and Heal**T**h **A**na**L**ytics (**ViTAL**) Lab at Emory Biomedical Informatics strives to develop artificial intelligence (AI) systems that are inclusive, accessible, fair, and reliable that will effectively improve the healthcare system. Our mission is to develop Ubiquitous Computing, Computer Vision, and Machine Learning systems using distributed ambient, mobile, and wearable devices to monitor patients' conditions in hospitals or everyday life. We are also invested in deploying and testing the developed AI systems in real-world clinical and daily living environments actively collaborating with stakeholders in healthcare.">
<meta property="og:url" content="">
<meta property="og:image" content="/images/share.png">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Research">
<meta property="twitter:description" content="Computational Beha**Vi**or and Heal**T**h **A**na**L**ytics (**ViTAL**) Lab at Emory Biomedical Informatics strives to develop artificial intelligence (AI) systems that are inclusive, accessible, fair, and reliable that will effectively improve the healthcare system. Our mission is to develop Ubiquitous Computing, Computer Vision, and Machine Learning systems using distributed ambient, mobile, and wearable devices to monitor patients' conditions in hospitals or everyday life. We are also invested in deploying and testing the developed AI systems in real-world clinical and daily living environments actively collaborating with stakeholders in healthcare.">
<meta property="twitter:url" content="">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/images/share.png">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Research",
    "description": "Computational Beha**Vi**or and Heal**T**h **A**na**L**ytics (**ViTAL**) Lab at Emory Biomedical Informatics strives to develop artificial intelligence (AI) systems that are inclusive, accessible, fair, and reliable that will effectively improve the healthcare system. Our mission is to develop Ubiquitous Computing, Computer Vision, and Machine Learning systems using distributed ambient, mobile, and wearable devices to monitor patients' conditions in hospitals or everyday life. We are also invested in deploying and testing the developed AI systems in real-world clinical and daily living environments actively collaborating with stakeholders in healthcare.",
    "headline": "Research",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/images/icon.png" }
    },
    "url": ""
  }
</script>

<link rel="alternate" type="application/rss+xml" href="/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.4.2/css/all.css" rel="preload" as="style" onload="this.onload = null; this.rel = 'stylesheet';">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.4.2/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/_styles/all.css" rel="stylesheet">
  

  
    <link href="/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/_styles/background.css" rel="stylesheet">
  

  
    <link href="/_styles/body.css" rel="stylesheet">
  

  
    <link href="/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/_styles/button.css" rel="stylesheet">
  

  
    <link href="/_styles/card.css" rel="stylesheet">
  

  
    <link href="/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/_styles/code.css" rel="stylesheet">
  

  
    <link href="/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/_styles/float.css" rel="stylesheet">
  

  
    <link href="/_styles/font.css" rel="stylesheet">
  

  
    <link href="/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/_styles/form.css" rel="stylesheet">
  

  
    <link href="/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/_styles/header.css" rel="stylesheet">
  

  
    <link href="/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/_styles/image.css" rel="stylesheet">
  

  
    <link href="/_styles/link.css" rel="stylesheet">
  

  
    <link href="/_styles/list.css" rel="stylesheet">
  

  
    <link href="/_styles/main.css" rel="stylesheet">
  

  
    <link href="/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/_styles/section.css" rel="stylesheet">
  

  
    <link href="/_styles/table.css" rel="stylesheet">
  

  
    <link href="/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/_scripts/anchors.js"></script>

  <script src="/_scripts/dark-mode.js"></script>

  <script src="/_scripts/fetch-tags.js"></script>

  <script src="/_scripts/search.js"></script>

  <script src="/_scripts/site-search.js"></script>

  <script src="/_scripts/tooltip.js"></script>


</head>

  <body>
    




<header class="background" style="--image: url('')" data-dark="false">
  <a href="/" class="home">
    
    
      <span class="title" data-tooltip="Home">
        
          <span>ViTAL Lab</span>
        
        
          <span>Computational Behavior and Health Analytics</span>
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/research/" data-tooltip="Research Projects">
          Research
        </a>
      
    
      
        <a href="/publications/" data-tooltip="Published works">
          Publications
        </a>
      
    
      
        <a href="/teaching/" data-tooltip="Courses">
          Teaching
        </a>
      
    
      
        <a href="/career/" data-tooltip="Career">
          Join
        </a>
      
    
      
        <a href="/contact/" data-tooltip="Email, address, and location">
          Contact
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - wrap each table in div to allow for scrolling
  - filter out blank sections
-->








  
  
  

  <section class="background" data-size="page">
    <h1 class="center" id="research-projects">Research Projects</h1>

<div class="button-wrapper">
    <a class="button" href="/research/#generative-ai-and-cross-modality-techniques-for-human-activity-recognition" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Human Activity Recognition</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#behavior-sensing-in-clinics-using-edge-and-cloud-ai-with-video-audio-and-wearable-sensors" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Clinics with Multimodal AI</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#ethical-artificial-intelligence-for-remote-interviews-or-wearables" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Telehealth with Ethical AI</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#mobile-computer-vision-and-closed-loop-intervention-systems-for-motion-assessments-and-rehabilitation" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Mobile Computer Vision</span>
      
    </a>
  </div>

<!-- [Machine Learning for Human Activity Recognition](#machine-learning-for-human-activity-recognition) 

[Edge Computing and Machine Learning Framework Using Multi-modal Ambient, Mobile, and Wearable Sensors](#edge-computing-and-machine-learning-framework-using-multi-modal-ambient-mobile-and-wearable-sensors)

[Behavior Analytics for Health Assessments](#behavior-analytics-for-health-assessments)
-->
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="generative-ai-and-cross-modality-techniques-for-human-activity-recognition">Generative AI and Cross-modality Techniques for Human Activity Recognition</h2>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/crossmodalHAR.png" style="
        width: 80%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>In addressing the challenges of building a human activity recognition model with limited wearable sensor data, we proposed a novel method utilizing vast video and textual data resources to generate a comprehensive training dataset. This cross-modality transfer learning technique harnesses state-of-the-art computer vision algorithms to interpret human motions from videos or Large Language Models to create text descriptions of human activities, eventually generating virtual inertial measurement signals for model training. The resulting model, trained on this rich dataset, can tackle real-world applications. Additionally, we introduced an innovative training scheme to account for the inherent uncertainties in activity annotations. This is complemented by a novel sensor feature representation that effectively captures the structural and distributional nuances of sensor signals, capturing human activity, enhancing the model’s robustness.</p>

<ul>
  <li>Kwon, H., Tong, C., Haresamudram, H., Gao, Y., Abowd, G. D., Lane, N. D., &amp; Ploetz, T. (2020). IMUTube: Automatic extraction of virtual on-body accelerometry from video for human activity recognition. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(3), 1-29.</li>
  <li>Zikang Leng, Amitrajit Bhattacharjee, Hrudhai Rajasekhar, Lizhe Zhang, Elizabeth Bruda, Hyeokhyen Kwon, and Thomas Plötz. 2024. IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based Human Activity Recognition. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 8, 3, Article 112 (August 2024), 32 pages. https://doi.org/10.1145/3678545</li>
  <li>Kwon, H., Abowd, G. D., &amp; Plötz, T. (2018, October). Adding structural characteristics to distribution-based accelerometer representations for activity recognition using wearables. In Proceedings of the 2018 ACM international symposium on wearable computers (pp. 72-75).</li>
  <li>Kwon, H., Abowd, G. D., &amp; Plötz, T. (2019, September). Handling annotation uncertainty in human activity recognition. In Proceedings of the 23rd International Symposium on Wearable Computers (pp. 109-117).</li>
</ul>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="behavior-sensing-in-clinics-using-edge-and-cloud-ai-with-video-audio-and-wearable-sensors">Behavior Sensing in Clinics using Edge and Cloud AI with Video, Audio, and Wearable Sensors</h2>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/cep_ep6_cam_wear.png" style="
        width: 80%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Neurological disorders impact mobility and social interaction, yet continuous and passive monitoring and assessment remain difficult, often relying on clinicians’ observations. To address this, a cost-effective multimodal monitoring system was developed using edge computing, AI, cameras, microphones and wearables to track movement and social behavior in therapeutic spaces, particularly for Mild Cognitive Impairment (MCI). Similarly, in autism spectrum disorder (ASD), video and wearable-based systems have shown promise in detecting challenging behaviors like aggression in classrooms, while preserving privacy. For Parkinson’s disease (PD), explainable AI models now offer objective assessments of motor subtypes, tremors, and Freezing of Gait (FOG), surpassing traditional subjective methods and potentially uncovering new biomarkers for early diagnosis and clinical research.</p>

<ul>
  <li>Hegde, C., Kiarashi, Y., Levey, A. I., Rodriguez, A. D., Kwon, H., &amp; Clifford, G. D. (2025). Feasibility of assessing cognitive impairment via distributed camera network and privacy‐preserving edge computing. Alzheimer’s &amp; Dementia: Diagnosis, Assessment &amp; Disease Monitoring, 17(1), e70085.</li>
  <li>Barun Das, Conor Anderson, Tania Villavicencio, Johanna Lantz, Jenny Foster, Theresa Hamlin, Ali Bahrami Rad, Gari D. Clifford, Hyeokhyen Kwon (2024). Explainable Artificial Intelligence for Quantifying Interfering and High-Risk Behaviors in Autism Spectrum Disorder in a Real-World Classroom Environment Using Privacy-Preserving Video Analysis. arXiv:2407.21691</li>
  <li>Rad, A. B., Villavicencio, T., Kiarashi, Y., Anderson, C., Foster, J., Kwon, H., … &amp; Clifford, G. D. (2025). From motion to emotion: exploring challenging behaviors in autism spectrum disorder through analysis of wearable physiology and movement. Physiological Measurement, 13(1), 015004.</li>
  <li>Kwon, H., Clifford, G. D., Genias, I., Bernhard, D., Esper, C. D., Factor, S. A., &amp; McKay, J. L. (2023). An explainable spatial-temporal graphical convolutional network to score freezing of gait in parkinsonian patients. Sensors, 23(4), 1766.</li>
</ul>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="ethical-artificial-intelligence-for-remote-interviews-or-wearables">Ethical Artificial Intelligence for Remote Interviews or Wearables</h2>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/ethical_ai.png" style="
        width: 80%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>We developed multimodal machine learning systems that analyze facial, vocal, linguistic, and cardiovascular signals from remote interviews to enhance psychiatric evaluations through objective digital biomarkers. While addressing demographic bias, the system effectively detects conditions like depression, cognitive impairment, social isolation, and psychological well-being. We also developed wearable-based models for monitoring gaits in Parkinson’s disease with transfer learning from diverse datasets, improving generalizability for varying demographics and disease conditions.</p>

<ul>
  <li>Jiang, Z., Seyedi, S., Griner, E., Abbasi, A., Rad, A.B., Kwon, H., Cotes, R.O. and Clifford, G.D., 2024. Multimodal Mental Health Digital Biomarker Analysis from Remote Interviews using Facial, Vocal, Linguistic, and Cardiovascular Patterns. IEEE Journal of Biomedical and Health Informatics.</li>
  <li>Jiang, Z., Seyedi, S., Griner, E., Abbasi, A., Rad, A.B., Kwon, H., Cotes, R.O. and Clifford, G.D. (2024) Evaluating and mitigating unfairness in multimodal remote mental health assessments. PLOS Digital Health 3(7): e0000413. https://doi.org/10.1371/journal.pdig.0000413</li>
  <li>Mu, X., Seyedi, S., Zheng, I., Jiang, Z., Chen, L., Omofojoye, B., … &amp; Kwon, H. (2024). Detecting Cognitive Impairment and Psychological Well-being among Older Adults Using Facial, Acoustic, Linguistic, and Cardiovascular Patterns Derived from Remote Conversations. arXiv preprint arXiv:2412.14194.</li>
  <li>Timothy Odonga, Christine D. Esper, Stewart A. Factor, J. Lucas McKay, Hyeokhyen Kwon. “On the Bias, Fairness, and Bias Mitigation for a Wearable-based Freezing of Gait Detection in Parkinson’s Disease.” arXiv preprint arXiv:2502.09626 (2025).</li>
</ul>

<h2 id="mobile-computer-vision-and-closed-loop-intervention-systems-for-motion-assessments-and-rehabilitation">Mobile Computer Vision and Closed-loop Intervention Systems for Motion Assessments and Rehabilitation</h2>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/mobileComputerVision.png" style="
        width: 60%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>We developed mobile AI systems to improve motor assessment and rehabilitation. A mobile phone-based, privacy-preserving system classifies various gait impairment patterns, demonstrating the potential for accessible gait analysis in clinical and tele-rehabilitation settings. We also developed a closed-loop neurostimulation system that integrates real-time video-based movement classification on webcam with wireless transcutaneous vagus nerve stimulation (tVNS), automatically triggering stimulation during rehabilitation exercises, showcasing a promising step toward patient-driven, home-based motor rehabilitation.</p>

<ul>
  <li>Reddy, Lauhitya, Ketan Anand, Shoibolina Kaushik, Corey Rodrigo, J. Lucas McKay, Trisha M. Kesar, and Hyeokhyen Kwon. “Classifying Simulated Gait Impairments using Privacy-preserving Explainable Artificial Intelligence and Mobile Phone Videos.” arXiv preprint arXiv:2412.01056 (2024).</li>
  <li>Minoru Shinohara, Arya Mohan, Nathaniel Green, Joshua N. Posen, Milka Trajkova, Woon-Hong Yeo, Hyeokhyen Kwon, “Closed-loop Neuromotor Training System Pairing Transcutaneous Vagus Nerve Stimulation with Video-based Real-time Movement Classification”, medRxiv 2025.05.23.25327218; doi: https://doi.org/10.1101/2025.05.23.25327218</li>
</ul>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->
  </section>


    </main>
    


<footer class="background" style="--image: url('')" data-dark="false" data-size="wide">
  <div>
    <!--
      Extra details like contact info or address
    -->

    <img src="../images/EU_shield_hz_280.jpg" height="50">
           
    <img src="../images/GeorgiaTech_RGB.png" height="60">
    <!-- &nbsp; &nbsp; &nbsp; &nbsp;
    <img
    src="../images/GaTechEmory_WHBME.jpg"
    height="60"
    >
    &nbsp; &nbsp; &nbsp; &nbsp;
    <img
    src="../images/BMI_hori.png"
    height="60"
    >
    &nbsp; &nbsp; &nbsp; &nbsp;
    <img
    src="../images/bme-logo-gold_0.png"
    height="60"
    > -->

  </div>

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="mailto:hyeokhyen.kwon@emory.edu" data-tooltip="Email" data-style="bare" aria-label="Email">
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://orcid.org/0000-0002-5693-3278" data-tooltip="ORCID" data-style="bare" aria-label="ORCID">
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=OwLXC4YAAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/KwonVitalLab" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://twitter.com/HyeokhyenKwon" data-tooltip="X" data-style="bare" aria-label="X">
      <i class="icon fa-brands fa-x-twitter"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2025
    ViTAL Lab
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
