<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  

























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Research | ViTAL Lab</title>

<link rel="icon" href="/images/icon.png">

<meta name="title" content="Research">
<meta name="description" content="We strive to develop artificial intelligence system that is inclusive, accessible, fair, and reliable that will effectively improve healthcare system. Our mission is to tackle mental and brain helth challenges through the developement of efficient computing and machine learning system on the network edge using distributed ambient, mobile, and edge devices to monitor patients' behaviors in everyday life.">

<meta property="og:title" content="Research">
<meta property="og:site_title" content="ViTAL Lab">
<meta property="og:description" content="We strive to develop artificial intelligence system that is inclusive, accessible, fair, and reliable that will effectively improve healthcare system. Our mission is to tackle mental and brain helth challenges through the developement of efficient computing and machine learning system on the network edge using distributed ambient, mobile, and edge devices to monitor patients' behaviors in everyday life.">
<meta property="og:url" content="">
<meta property="og:image" content="/images/share.png">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Research">
<meta property="twitter:description" content="We strive to develop artificial intelligence system that is inclusive, accessible, fair, and reliable that will effectively improve healthcare system. Our mission is to tackle mental and brain helth challenges through the developement of efficient computing and machine learning system on the network edge using distributed ambient, mobile, and edge devices to monitor patients' behaviors in everyday life.">
<meta property="twitter:url" content="">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/images/share.png">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Research",
    "description": "We strive to develop artificial intelligence system that is inclusive, accessible, fair, and reliable that will effectively improve healthcare system. Our mission is to tackle mental and brain helth challenges through the developement of efficient computing and machine learning system on the network edge using distributed ambient, mobile, and edge devices to monitor patients' behaviors in everyday life.",
    "headline": "Research",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/images/icon.png" }
    },
    "url": ""
  }
</script>

<link rel="alternate" type="application/rss+xml" href="/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.4.2/css/all.css" rel="preload" as="style" onload="this.onload = null; this.rel = 'stylesheet';">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.4.2/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/_styles/all.css" rel="stylesheet">
  

  
    <link href="/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/_styles/background.css" rel="stylesheet">
  

  
    <link href="/_styles/body.css" rel="stylesheet">
  

  
    <link href="/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/_styles/button.css" rel="stylesheet">
  

  
    <link href="/_styles/card.css" rel="stylesheet">
  

  
    <link href="/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/_styles/code.css" rel="stylesheet">
  

  
    <link href="/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/_styles/float.css" rel="stylesheet">
  

  
    <link href="/_styles/font.css" rel="stylesheet">
  

  
    <link href="/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/_styles/form.css" rel="stylesheet">
  

  
    <link href="/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/_styles/header.css" rel="stylesheet">
  

  
    <link href="/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/_styles/image.css" rel="stylesheet">
  

  
    <link href="/_styles/link.css" rel="stylesheet">
  

  
    <link href="/_styles/list.css" rel="stylesheet">
  

  
    <link href="/_styles/main.css" rel="stylesheet">
  

  
    <link href="/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/_styles/section.css" rel="stylesheet">
  

  
    <link href="/_styles/table.css" rel="stylesheet">
  

  
    <link href="/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/_scripts/anchors.js"></script>

  <script src="/_scripts/dark-mode.js"></script>

  <script src="/_scripts/fetch-tags.js"></script>

  <script src="/_scripts/search.js"></script>

  <script src="/_scripts/site-search.js"></script>

  <script src="/_scripts/tooltip.js"></script>


<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

  <body>
    




<header class="background" style="--image: url('')" data-dark="false">
  <a href="/" class="home">
    
    
      <span class="title" data-tooltip="Home">
        
          <span>ViTAL Lab</span>
        
        
          <span>Machine Learning for Behavior and Health Analytics</span>
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/research/" data-tooltip="Research Projects">
          Research
        </a>
      
    
      
        <a href="/publications/" data-tooltip="Published works">
          Publications
        </a>
      
    
      
        <a href="/teaching/" data-tooltip="Courses">
          Teaching
        </a>
      
    
      
        <a href="/career/" data-tooltip="Musings and miscellany">
          Join
        </a>
      
    
      
        <a href="/contact/" data-tooltip="Email, address, and location">
          Contact
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - wrap each table in div to allow for scrolling
  - filter out blank sections
-->








  
  
  

  <section class="background" data-size="page">
    <h1 class="center" id="research-projects">Research Projects</h1>

<div class="button-wrapper">
    <a class="button" href="/research/#machine-learning-for-human-activity-recognition" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Human Activity Recognition</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#edge-computing-and-machine-learning-framework-using-multi-modal-ambient-mobile-and-wearable-sensors" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Machine Learning with Edge and Cloud Computing</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#behavior-analytics-for-health-assessments" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Health Analytics</span>
      
    </a>
  </div>

<!-- [Machine Learning for Human Activity Recognition](#machine-learning-for-human-activity-recognition) 

[Edge Computing and Machine Learning Framework Using Multi-modal Ambient, Mobile, and Wearable Sensors](#edge-computing-and-machine-learning-framework-using-multi-modal-ambient-mobile-and-wearable-sensors)

[Behavior Analytics for Health Assessments](#behavior-analytics-for-health-assessments)
-->
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="machine-learning-for-human-activity-recognition">Machine Learning for Human Activity Recognition</h2>

<h3 id="opportunistic-use-of-video-data-for-wearable-based-human-activity-recognition">Opportunistic Use of Video Data for Wearable-based Human Activity Recognition</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/IMUTube%20Diagrams.jpg" style="
        width: 50%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Building human activity recognition model involves training a machine learning model to classify the given sensor signal input for target activities. The performance of the supervised learning approach largely depends on the scale of the training dataset that covers diverse activity patterns in the wild. However, collecting wearable sensor data from users is expensive, and annotation is time-consuming. Thereby, wearable datasets are typically limited in scale. To overcome the challenges in collecting wearable datasets, We proposed a method that uses a virtually unlimited amount of video data in online repositories to generate a training sensor dataset. The proposed technique uses state-of-the-art computer vision algorithms to track full human motions from human activity video and extract virtual inertial measurement signals from the on-body locations. The collection of virtual sensors from target activity videos is used to train the human activity recognition model, which is deployed in the real world. The proposed technique opens up the opportunity to apply complex analysis models for human activity recognition with the availability of large-scale training data</p>

<ul>
  <li>Kwon, H., Tong, C., Haresamudram, H., Gao, Y., Abowd, G. D., Lane, N. D., &amp; Ploetz, T. (2020). IMUTube: Automatic extraction of virtual on-body accelerometry from video for human activity recognition. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(3), 1-29.</li>
  <li>Kwon, H., Wang, B., Abowd, G. D., &amp; Plötz, T. (2021). Approaching the Real-World: Supporting Activity Recognition Training with Virtual IMU Data. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 5(3), 1-32.</li>
  <li>Kwon, H., Abowd, G. D., &amp; Plötz, T. (2021). Complex Deep Neural Networks from Large Scale Virtual IMU Data for Effective Human Activity Recognition Using Wearables. Sensors, 21(24), 8337.</li>
</ul>

<h3 id="large-language-model-and-generative-motion-model-for-wearable-based-human-activity-recognition">Large Language Model and Generative Motion Model for Wearable-based Human Activity Recognition</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/gpt-iswc.png" style="
        width: 50%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>The development of robust, generalized models in human activity recognition (HAR) has been hindered by the scarcity of large-scale, labeled data sets. Recent work has shown that virtual IMU data extracted from videos using computer vision techniques can lead to substantial performance improvements when training HAR models combined with small portions of real IMU data. Inspired by recent advances in motion synthesis from textual descriptions and connecting Large Language Models (LLMs) to various AI models, we introduce an automated pipeline that first uses ChatGPT to generate diverse textual descriptions of activities. These textual descriptions are then used to generate 3D human motion sequences via a motion synthesis model, T2M-GPT, and later converted to streams of virtual IMU data. We benchmarked our approach on three HAR datasets (RealWorld, PAMAP2, and USC-HAD) and demonstrate that the use of virtual IMU training data generated using our new approach leads to significantly improved HAR model performance compared to only using real IMU data. Our approach contributes to the growing field of cross-modality transfer methods and illustrate how HAR models can be improved through the generation of virtual training data that do not require any manual effort.</p>

<ul>
  <li>Zikang Leng, Hyeokhyen Kwon, and Thomas Ploetz. 2023. Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition. In Proceedings of the 2023 International Symposium on Wearable Computers (ISWC ‘23). Association for Computing Machinery, New York, NY, USA, 39–43. https://doi.org/10.1145/3594738.3611361</li>
</ul>

<h3 id="robust-human-activity-recognition-model-with-uncertainty-modeling">Robust Human Activity Recognition Model with Uncertainty Modeling</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/label%20jitter.png" style="
        width: 80%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Building human activity recognition model involves training a machine learning model to classify the given sensor signal feature for target activities. For the conventional supervised learning approach, discriminative sensor feature representation and accurate activity annotations are necessary to learn the robust human activity recognition model. However, acquiring accurate activity annotation is intractable due to the continuous nature of human activities. Thereby, the activity annotations have uncertainty regarding the correct temporal alignment of activity boundaries and the correctness of the actual label. We explore novel machine learning techniques to handle such uncertainties.</p>

<ul>
  <li>Kwon, H., Abowd, G. D., &amp; Plötz, T. (2018, October). Adding structural characteristics to distribution-based accelerometer representations for activity recognition using wearables. In Proceedings of the 2018 ACM international symposium on wearable computers (pp. 72-75).</li>
  <li>Kwon, H., Abowd, G. D., &amp; Plötz, T. (2019, September). Handling annotation uncertainty in human activity recognition. In Proceedings of the 23rd International Symposium on Wearable Computers (pp. 109-117).</li>
</ul>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="edge-computing-and-machine-learning-framework-using-multi-modal-ambient-mobile-and-wearable-sensors">Edge Computing and Machine Learning Framework Using Multi-modal Ambient, Mobile, and Wearable Sensors</h2>

<h3 id="distributed-multi-view-camera-system-for-behavior-recognition-using-edge-fog-and-cloud-computing-system">Distributed Multi-view Camera System for Behavior Recognition Using Edge, Fog, and Cloud Computing System.</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/cep_ep6_camera_mot.png" style="
        width: 60%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Localization of individuals in a built environment is a growing research topic. Estimating the positions, face orientation (or gaze direction) and trajectories of people through space has many uses, such as in crowd management, security, and healthcare. In this work, we present an open-source, low-cost, scalable and privacy-preserving edge computing framework for multi-person localization, i.e. estimating the positions, orientations, and trajectories of multiple people in an indoor space. Our computing framework consists of 38 Tensor Processing Unit (TPU)-enabled edge computing camera systems placed in the ceiling of the indoor therapeutic space. The edge compute systems are connected to an on-premise fog server through a secure and private network. A multi-person detection algorithm and a pose estimation model run on the edge TPU in real-time to collect features which are used, instead of raw images, for downstream computations. This ensures the privacy of individuals in the space, reduces data transmission/storage and improves scalability. We implemented a Kalman filter-based multi-person tracking method and a state-of-the-art body orientation estimation method to determine the positions and facing orientations of multiple people simultaneously in the indoor space. For our study site with size of 18,000 square feet, our system demonstrated an average localization error of 1.41 meters, a multiple-object tracking accuracy score of 62%, and a mean absolute body orientation error of 29°, which is sufficient for understanding group activity behaviors in indoor environments. Additionally, our study provides practical guidance for deploying the proposed system by analyzing various elements of the camera installation with respect to tracking accuracy.</p>

<ul>
  <li>H. Kwon et al., “A Feasibility Study on Indoor Localization and Multi-person Tracking Using Sparsely Distributed Camera Network with Edge Computing,” in IEEE Journal of Indoor and Seamless Positioning and Navigation, doi: 10.1109/JISPIN.2023.3337189.</li>
</ul>

<h3 id="wearable-based-behavior-recognition-using-edge-fog-and-cloud-computing-system">Wearable-based Behavior Recognition Using Edge, Fog, and Cloud Computing System</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/cep_ep6_ble_imu_mot.png" style="
        width: 40%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Spatial navigation of indoor space usage patterns reveals important cues about the cognitive health of individuals. In this work, we present a low-cost, scalable, open-source edge computing system using Bluetooth Low Energy (BLE) and Inertial Measurement Unit sensors (IMU) for tracking indoor movements for a large indoor facility (over 1600 $m^2$) that was designed to facilitate therapeutic activities for individuals with Mild Cognitive Impairment. The facility is instrumented with 39 edge computing systems with an on-premise fog server, and subjects carry BLE beacon and IMU sensors on-body. We proposed an adaptive trilateration approach that considers the temporal density of hits from the BLE beacon to surrounding edge devices to handle inconsistent coverage of edge devices in large spaces with varying signal strength that leads to intermittent detection of beacons. The proposed BLE-based localization is further enhanced by fusing with an IMU-based tracking method using a dead-reckoning technique. Our experiment results, achieved in a real clinical environment, suggest that an ordinary medical facility can be transformed into a smart space that enables automatic assessment of the individual patients’ movements.</p>

<ul>
  <li>Kiarashi, Y.; Saghafi, S.; Das, B.; Hegde, C.; Madala, V.S.K.; Nakum, A.; Singh, R.; Tweedy, R.; Doiron, M.; Rodriguez, A.D.; et al. Graph Trilateration for Indoor Localization in Sparsely Distributed Edge Computing Devices in Complex Environments Using Bluetooth Technology. Sensors 2023, 23, 9517.</li>
</ul>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="behavior-analytics-for-health-assessments">Behavior Analytics for Health Assessments</h2>

<h3 id="movement-disorder-assessment-with-kinematics-dataset">Movement Disorder Assessment with Kinematics Dataset</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/xai4fog.png" style="
        width: 60%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Freezing of gait (FOG) is a poorly understood heterogeneous gait disorder seen in patients with parkinsonism which contributes to significant morbidity and social isolation. FOG is currently measured with scales that are typically performed by movement disorders specialists (i.e., MDS-UPDRS), or through patient-completed questionnaires (N-FOG-Q), both of which are inadequate in addressing the heterogeneous nature of the disorder and are unsuitable for use in clinical trials. Moreover, prior studies that investigated machine-learning-based techniques to objectively phenotype FOG primarily used spectral analyses of kinematic data from few on-body locations during gait assessments, which lack details to capture complex whole-body movements. To overcome those limitations, we devise a method using explainable artificial intelligence models that can measure FOG objectively and accurately, hence improving our ability to identify it and accurately evaluate new therapies. The proposed model was able to explain how kinematic movements are associated with each FOG severity level that were highly consistent with the features, in which movement disorders specialists are trained to identify as characteristics of freezing. Overall, the proposed technique demonstrates that deep learning models’ capability to capture complex movement patterns in kinematic data can automatically and objectively score FOG with high accuracy. These models have the potential to discover novel kinematic biomarkers for FOG that can be used for hypothesis generation and potentially as clinical trial outcome measures.</p>

<ul>
  <li>Kwon, H., Clifford, G. D., Genias, I., Bernhard, D., Esper, C. D., Factor, S. A., &amp; McKay, J. L. (2023). An explainable spatial-temporal graphical convolutional network to score freezing of gait in parkinsonian patients. Sensors, 23(4), 1766.</li>
</ul>

<h3 id="mental-health-assessment-with-remote-interviews">Mental Health Assessment with Remote Interviews</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/multi-modal-MH.png" style="
        width: 60%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>The current clinical practice of psychiatric evaluation suffers from subjectivity and bias, and requires highly skilled professionals that are often unavailable or unaffordable. Objective digital biomarkers have shown the potential to address these issues. In this work, we investigated whether behavioral and physiological signals, extracted from remote interviews, provided complimentary information for assessing psychiatric disorders. Time series of multimodal features were derived from four conceptual modes: facial expression, vocal expression, linguistic expression, and cardiovascular modulation. The features were extracted from simultaneously recorded audio and video of remote interviews using task-specific and foundation models. Averages, standard deviations, and hidden Markov model-derived statistics of these features were computed from 73 subjects. Four binary classification tasks were defined: detecting 1) any clinically-diagnosed psychiatric disorder, 2) major depressive disorder, 3) self-rated depression, and 4) self-rated anxiety. Each modality was evaluated individually and in combination. Multimodal features extracted from remote interviews revealed informative characteristics of clinically diagnosed and self-rated mental health status. The proposed multimodal approach has the potential to facilitate objective, remote, and low-cost assessment for low-burden automated mental health services.</p>

<ul>
  <li>Jiang, Zifan, Salman Seyedi, Emily Lynn Griner, Ahmed Abbasi, Ali Bahrami Rad, Hyeokhyen Kwon, Robert O. Cotes, and Gari D. Clifford. “Multimodal mental health assessment with remote interviews using facial, vocal, linguistic, and cardiovascular patterns.” medRxiv (2023): 2023-09.</li>
</ul>

<h3 id="cognitive-impairment">Cognitive Impairment</h3>

<p>Soon to come. <em>Stay tuned</em></p>

<h3 id="autism-spectrum-disorder">Autism Spectrum Disorder</h3>

<p>Soon to come. <em>Stay tuned</em></p>
  </section>


    </main>
    


<footer class="background" style="--image: url('')" data-dark="false" data-size="wide">
  <div>
    <!--
      Extra details like contact info or address
    -->

    <img src="../images/EU_shield_hz_280.jpg" height="50">
           
    <img src="../images/GeorgiaTech_RGB.png" height="60">


  </div>

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="mailto:hyeokhyen.kwon@emory.edu" data-tooltip="Email" data-style="bare" aria-label="Email">
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://orcid.org/0000-0002-5693-3278" data-tooltip="ORCID" data-style="bare" aria-label="ORCID">
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=OwLXC4YAAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/KwonVitalLab" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://twitter.com/HyeokhyenKwon" data-tooltip="X/Twitter" data-style="bare" aria-label="X/Twitter">
      <i class="icon fa-brands fa-x-twitter"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2023
    ViTAL Lab
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
