<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  

























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Research | ViTAL Lab</title>

<link rel="icon" href="/images/icon.png">

<meta name="title" content="Research">
<meta name="description" content="We strive to develop artificial intelligence system that is inclusive, accessible, fair, and reliable that will effectively improve healthcare system. Our mission is to tackle mental and brain helth challenges through the developement of efficient computing and machine learning system on the network edge using distributed ambient, mobile, and edge devices to monitor patients' behaviors in everyday life.">

<meta property="og:title" content="Research">
<meta property="og:site_title" content="ViTAL Lab">
<meta property="og:description" content="We strive to develop artificial intelligence system that is inclusive, accessible, fair, and reliable that will effectively improve healthcare system. Our mission is to tackle mental and brain helth challenges through the developement of efficient computing and machine learning system on the network edge using distributed ambient, mobile, and edge devices to monitor patients' behaviors in everyday life.">
<meta property="og:url" content="">
<meta property="og:image" content="/images/share.png">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Research">
<meta property="twitter:description" content="We strive to develop artificial intelligence system that is inclusive, accessible, fair, and reliable that will effectively improve healthcare system. Our mission is to tackle mental and brain helth challenges through the developement of efficient computing and machine learning system on the network edge using distributed ambient, mobile, and edge devices to monitor patients' behaviors in everyday life.">
<meta property="twitter:url" content="">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/images/share.png">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Research",
    "description": "We strive to develop artificial intelligence system that is inclusive, accessible, fair, and reliable that will effectively improve healthcare system. Our mission is to tackle mental and brain helth challenges through the developement of efficient computing and machine learning system on the network edge using distributed ambient, mobile, and edge devices to monitor patients' behaviors in everyday life.",
    "headline": "Research",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/images/icon.png" }
    },
    "url": ""
  }
</script>

<link rel="alternate" type="application/rss+xml" href="/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.3.0/css/all.css" rel="preload" as="style" onload="this.onload = null; this.rel = 'stylesheet';">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.3.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/_styles/all.css" rel="stylesheet">
  

  
    <link href="/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/_styles/background.css" rel="stylesheet">
  

  
    <link href="/_styles/body.css" rel="stylesheet">
  

  
    <link href="/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/_styles/button.css" rel="stylesheet">
  

  
    <link href="/_styles/card.css" rel="stylesheet">
  

  
    <link href="/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/_styles/code.css" rel="stylesheet">
  

  
    <link href="/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/_styles/float.css" rel="stylesheet">
  

  
    <link href="/_styles/font.css" rel="stylesheet">
  

  
    <link href="/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/_styles/form.css" rel="stylesheet">
  

  
    <link href="/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/_styles/header.css" rel="stylesheet">
  

  
    <link href="/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/_styles/image.css" rel="stylesheet">
  

  
    <link href="/_styles/link.css" rel="stylesheet">
  

  
    <link href="/_styles/list.css" rel="stylesheet">
  

  
    <link href="/_styles/main.css" rel="stylesheet">
  

  
    <link href="/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/_styles/section.css" rel="stylesheet">
  

  
    <link href="/_styles/table.css" rel="stylesheet">
  

  
    <link href="/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/_scripts/anchors.js"></script>

  <script src="/_scripts/dark-mode.js"></script>

  <script src="/_scripts/fetch-tags.js"></script>

  <script src="/_scripts/search.js"></script>

  <script src="/_scripts/site-search.js"></script>

  <script src="/_scripts/tooltip.js"></script>


</head>

  <body>
    




<header class="background" style="--image: url('')" data-dark="false">
  <a href="/" class="home">
    
    
      <span class="title" data-tooltip="Home">
        
          <span>ViTAL Lab</span>
        
        
          <span>Machine Learning for Behavior and Health Analytics</span>
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/research/" data-tooltip="Research Projects">
          Research
        </a>
      
    
      
        <a href="/publications/" data-tooltip="Published works">
          Publications
        </a>
      
    
      
        <a href="/teaching/" data-tooltip="Courses">
          Teaching
        </a>
      
    
      
        <a href="/career/" data-tooltip="Musings and miscellany">
          Career
        </a>
      
    
      
        <a href="/contact/" data-tooltip="Email, address, and location">
          Contact
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - wrap each table in div to allow for scrolling
  - filter out blank sections
-->








  
  
  

  <section class="background" data-size="page">
    <h1 class="center" id="research-projects">Research Projects</h1>

<div class="button-wrapper">
    <a class="button" href="/research/#machine-learning-for-human-activity-recognition" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Human Activity Recognition</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#edge-computing-and-machine-learning-framework-using-multi-modal-ambient-mobile-and-wearable-sensors" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Machine Learning on Edge</span>
      
    </a>
  </div>

<div class="button-wrapper">
    <a class="button" href="/research/#behavior-analytics-for-health-assessments" data-tooltip="Link" data-style="" aria-label="Link">
      
      
        <span>Health Analytics</span>
      
    </a>
  </div>

<!-- [Machine Learning for Human Activity Recognition](#machine-learning-for-human-activity-recognition) 

[Edge Computing and Machine Learning Framework Using Multi-modal Ambient, Mobile, and Wearable Sensors](#edge-computing-and-machine-learning-framework-using-multi-modal-ambient-mobile-and-wearable-sensors)

[Behavior Analytics for Health Assessments](#behavior-analytics-for-health-assessments)
-->
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="machine-learning-for-human-activity-recognition">Machine Learning for Human Activity Recognition</h2>

<h3 id="opportunistic-use-of-video-data-for-wearable-based-human-activity-recognition">Opportunistic Use of Video Data for Wearable-based Human Activity Recognition</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/IMUTube%20Diagrams.jpg" style="
        width: 50%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Building human activity recognition model involves training a machine learning model to classify the given sensor signal input for target activities. The performance of the supervised learning approach largely depends on the scale of the training dataset that covers diverse activity patterns in the wild. However, collecting wearable sensor data from users is expensive, and annotation is time-consuming. Thereby, wearable datasets are typically limited in scale. To overcome the challenges in collecting wearable datasets, We proposed a method that uses a virtually unlimited amount of video data in online repositories to generate a training sensor dataset. The proposed technique uses state-of-the-art computer vision algorithms to track full human motions from human activity video and extract virtual inertial measurement signals from the on-body locations. The collection of virtual sensors from target activity videos is used to train the human activity recognition model, which is deployed in the real world. The proposed technique opens up the opportunity to apply complex analysis models for human activity recognition with the availability of large-scale training data</p>

<ul>
  <li>Kwon, H., Tong, C., Haresamudram, H., Gao, Y., Abowd, G. D., Lane, N. D., &amp; Ploetz, T. (2020). IMUTube: Automatic extraction of virtual on-body accelerometry from video for human activity recognition. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(3), 1-29.</li>
  <li>Kwon, H., Wang, B., Abowd, G. D., &amp; Plötz, T. (2021). Approaching the Real-World: Supporting Activity Recognition Training with Virtual IMU Data. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 5(3), 1-32.</li>
  <li>Kwon, H., Abowd, G. D., &amp; Plötz, T. (2021). Complex Deep Neural Networks from Large Scale Virtual IMU Data for Effective Human Activity Recognition Using Wearables. Sensors, 21(24), 8337.</li>
</ul>

<h3 id="robust-human-activity-recognition-model-with-wearable-sensors">Robust Human Activity Recognition Model with Wearable Sensors</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/label%20jitter.png" style="
        width: 80%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Building human activity recognition model involves training a machine learning model to classify the given sensor signal feature for target activities. For the conventional supervised learning approach, discriminative sensor feature representation and accurate activity annotations are necessary to learn the robust human activity recognition model. However, acquiring accurate activity annotation is intractable due to the continuous nature of human activities. Thereby, the activity annotations have uncertainty regarding the correct temporal alignment of activity boundaries and the correctness of the actual label. We explore novel machine learning techniques to handle such uncertainties.</p>

<ul>
  <li>Kwon, H., Abowd, G. D., &amp; Plötz, T. (2018, October). Adding structural characteristics to distribution-based accelerometer representations for activity recognition using wearables. In Proceedings of the 2018 ACM international symposium on wearable computers (pp. 72-75).</li>
  <li>Kwon, H., Abowd, G. D., &amp; Plötz, T. (2019, September). Handling annotation uncertainty in human activity recognition. In Proceedings of the 23rd International Symposium on Wearable Computers (pp. 109-117).</li>
</ul>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="edge-computing-and-machine-learning-framework-using-multi-modal-ambient-mobile-and-wearable-sensors">Edge Computing and Machine Learning Framework Using Multi-modal Ambient, Mobile, and Wearable Sensors</h2>

<p>Soon to come. <em>Stay tuned</em></p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="behavior-analytics-for-health-assessments">Behavior Analytics for Health Assessments</h2>

<h3 id="parkinsons-disease">Parkinson’s Disease</h3>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/images/research/xai4fog.png" style="
        width: 60%;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  
</figure>

<p>Freezing of gait (FOG) is a poorly understood heterogeneous gait disorder seen in patients with parkinsonism which contributes to significant morbidity and social isolation. FOG is currently measured with scales that are typically performed by movement disorders specialists (i.e., MDS-UPDRS), or through patient-completed questionnaires (N-FOG-Q), both of which are inadequate in addressing the heterogeneous nature of the disorder and are unsuitable for use in clinical trials. Moreover, prior studies that investigated machine-learning-based techniques to objectively phenotype FOG primarily used spectral analyses of kinematic data from few on-body locations during gait assessments, which lack details to capture complex whole-body movements. To overcome those limitations, we devise a method using explainable artificial intelligence models that can measure FOG objectively and accurately, hence improving our ability to identify it and accurately evaluate new therapies. The proposed model was able to explain how kinematic movements are associated with each FOG severity level that were highly consistent with the features, in which movement disorders specialists are trained to identify as characteristics of freezing. Overall, the proposed technique demonstrates that deep learning models’ capability to capture complex movement patterns in kinematic data can automatically and objectively score FOG with high accuracy. These models have the potential to discover novel kinematic biomarkers for FOG that can be used for hypothesis generation and potentially as clinical trial outcome measures.</p>

<ul>
  <li>Kwon, H., Clifford, G. D., Genias, I., Bernhard, D., Esper, C. D., Factor, S. A., &amp; McKay, J. L. (2023). An explainable spatial-temporal graphical convolutional network to score freezing of gait in parkinsonian patients. Sensors, 23(4), 1766.</li>
</ul>

<h3 id="cognitive-impairment">Cognitive Impairment</h3>

<p>Soon to come. <em>Stay tuned</em></p>

<h3 id="autism-spectrum-disorder">Autism Spectrum Disorder</h3>

<p>Soon to come. <em>Stay tuned</em></p>
  </section>


    </main>
    


<footer class="background" style="--image: url('')" data-dark="false" data-size="wide">
  <div>
    <!--
      Extra details like contact info or address
    -->

    <img src="../images/EU_shield_hz_280.jpg" height="50">
           
    <img src="../images/GeorgiaTech_RGB.png" height="60">


  </div>

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="mailto:hyeokhyen.kwon@dbmi.emory.edu" data-tooltip="Email" data-style="bare" aria-label="Email">
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://orcid.org/0000-0002-5693-3278" data-tooltip="ORCID" data-style="bare" aria-label="ORCID">
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=1t4fsxYAAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/KwonVitalLab" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://twitter.com/HyeokhyenKwon" data-tooltip="Twitter" data-style="bare" aria-label="Twitter">
      <i class="icon fa-brands fa-twitter"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="" data-tooltip="YouTube" data-style="bare" aria-label="YouTube">
      <i class="icon fa-brands fa-youtube"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2023
    ViTAL Lab
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
